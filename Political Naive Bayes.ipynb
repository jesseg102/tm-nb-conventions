{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes on Political Text\n",
    "\n",
    "In this notebook we use Naive Bayes to explore and classify political data. See the `README.md` for full details. You can download the required DB from the shared dropbox or from blackboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n",
      "[nltk_data] Error loading punkt: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1000)>\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# Feel free to include your text patterns functions\n",
    "#from text_functions_solutions import clean_tokenize, get_patterns\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "# Ensure that you have downloaded the stopwords and punkt data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to clean & tokenize text\n",
    "def clean_tokenize(text):\n",
    "    # Tokenize  text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Convert to lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    # Remove punctuation\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Return tokens as a single string\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "convention_db = sqlite3.connect(\"2020_Convention.db\")\n",
    "convention_cur = convention_db.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Exploratory Naive Bayes\n",
    "\n",
    "We'll first build a NB model on the convention data itself, as a way to understand what words distinguish between the two parties. This is analogous to what we did in the \"Comparing Groups\" class work. First, pull in the text \n",
    "for each party and prepare it for use in Naive Bayes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "convention_db = sqlite3.connect(\"2020_Conventions.db\")\n",
    "convention_cur = convention_db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in the database:\n",
      "conventions\n",
      "\n",
      "Columns in table conventions:\n",
      "party (TEXT)\n",
      "night (INTEGER)\n",
      "speaker (TEXT)\n",
      "speaker_count (INTEGER)\n",
      "time (TEXT)\n",
      "text (TEXT)\n",
      "text_len (TEXT)\n",
      "file (TEXT)\n"
     ]
    }
   ],
   "source": [
    "# Connect to database displaying all tables & columns\n",
    "try:\n",
    "    # List all tables in the database\n",
    "    tables_query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "    convention_cur.execute(tables_query)\n",
    "    tables = convention_cur.fetchall()\n",
    "\n",
    "    # Check if tables are found\n",
    "    if not tables:\n",
    "        print(\"No tables found in the database.\")\n",
    "    else:\n",
    "        print(\"Tables in the database:\")\n",
    "        for table in tables:\n",
    "            print(table[0])\n",
    "\n",
    "        # Describe each table & column names\n",
    "        for table in tables:\n",
    "            print(f\"\\nColumns in table {table[0]}:\")\n",
    "            convention_cur.execute(f\"PRAGMA table_info({table[0]});\")\n",
    "            columns = convention_cur.fetchall()\n",
    "            for column in columns:\n",
    "                print(f\"{column[1]} ({column[2]})\")\n",
    "\n",
    "except sqlite3.Error as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    if convention_db:\n",
    "        convention_db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "convention_db = sqlite3.connect(\"2020_Conventions.db\")\n",
    "convention_cur = convention_db.cursor()\n",
    "\n",
    "convention_data = []\n",
    "\n",
    "# Fill this list up with items that are themselves lists. The \n",
    "# first element in the sublist should be the cleaned and tokenized\n",
    "# text in a single string. As part of your cleaning process,\n",
    "# remove the stopwords from the text. The second element of the sublist\n",
    "# should be the party. \n",
    "\n",
    "query_results = convention_cur.execute(\n",
    "                            '''\n",
    "                            SELECT text, party \n",
    "                            FROM conventions; \n",
    "                            ''')\n",
    "\n",
    "for row in query_results:\n",
    "    cleaned_text = clean_tokenize(row[0])\n",
    "    party = row[1]\n",
    "    convention_data.append([cleaned_text, party])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some random entries and see if they look right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hello pastor jerry young let us pray together almighty god grateful granted nation democratic ideal destiny may fashioned thankful blessed united states america survive climate confusion chaos racism injustice uncertainty helplessness irresponsibility oh lord invoke presence participation throughout life convention prayer enable convention produce vision promote healing hope health nation vision inspire inform inclusive americans vision rekindle us renewed commitment high ideal democracy created equal endowed creator certain inalienable rights among life liberty pursuit happiness',\n",
       "  'Democratic'],\n",
       " ['time sleeping basement joe biden years produce results talk action like many democrats making promises black voters decades captive audience president trump sought earn black vote democratic party leaders went crazy nancy pelosi chuck schumer literally started wearing kente cloths around us capitol pandering enough keep us satisfied',\n",
       "  'Republican'],\n",
       " ['senator harris cares people doubt', 'Democratic'],\n",
       " ['know fabulous wait see debate current vice president mika pints paints',\n",
       "  'Democratic'],\n",
       " ['mean clear decided going senator boys need joe biden prepared walk away men like ted kennedy mike mansfield hubert humphrey fritz hollings danny inouye convinced stay stay six months remember danny stay six valerie biden owens allow suffering debilitate like allow stuttering define backbone something bigger joe suffering',\n",
       "  'Democratic']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choices(convention_data,k=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that looks good, we now need to make our function to turn these into features. In my solution, I wanted to keep the number of features reasonable, so I only used words that occur at least `word_cutoff` times. Here's the code to test that if you want it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With a word cutoff of 5, we have 2236 as features in the model.\n"
     ]
    }
   ],
   "source": [
    "word_cutoff = 5\n",
    "\n",
    "tokens = [w for t, p in convention_data for w in t.split()]\n",
    "\n",
    "word_dist = nltk.FreqDist(tokens)\n",
    "\n",
    "feature_words = set()\n",
    "\n",
    "for word, count in word_dist.items() :\n",
    "    if count > word_cutoff :\n",
    "        feature_words.add(word)\n",
    "        \n",
    "print(f\"With a word cutoff of {word_cutoff}, we have {len(feature_words)} as features in the model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_features(text, fw):\n",
    "    \"\"\"Given some text, this returns a dictionary holding the\n",
    "       feature words.\n",
    "       \n",
    "       Args: \n",
    "            * text: a piece of text in a continuous string. Assumes\n",
    "            text has been cleaned and case folded.\n",
    "            * fw: the *feature words* that we're considering. A word \n",
    "            in `text` must be in fw in order to be returned. This \n",
    "            prevents us from considering very rarely occurring words.\n",
    "        \n",
    "       Returns: \n",
    "            A dictionary with the words in `text` that appear in `fw`. \n",
    "            Words are only counted once. \n",
    "            If `text` were \"quick quick brown fox\" and `fw` = {'quick','fox','jumps'},\n",
    "            then this would return a dictionary of \n",
    "            {'quick' : True,\n",
    "             'fox' :    True}\n",
    "        \n",
    "    \"\"\"\n",
    "    # Initialize an empty dictionary to hold feature words found in the text\n",
    "    ret_dict = {}\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # Iterate through the tokens\n",
    "    for token in tokens:\n",
    "        # If the token is in the feature words set, add it to the dictionary\n",
    "        if token in fw:\n",
    "            ret_dict[token] = True\n",
    "    \n",
    "    return ret_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len(feature_words)>0)\n",
    "assert(conv_features(\"donald is the president\",feature_words)==\n",
    "       {'donald':True,'president':True})\n",
    "assert(conv_features(\"some people in america are citizens\",feature_words)==\n",
    "                     {'people':True,'america':True,\"citizens\":True})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll build our feature set. Out of curiosity I did a train/test split to see how accurate the classifier was, but we don't strictly need to since this analysis is exploratory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets = [(conv_features(text,feature_words), party) for (text, party) in convention_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(20220507)\n",
    "random.shuffle(featuresets)\n",
    "\n",
    "test_size = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.494\n"
     ]
    }
   ],
   "source": [
    "test_set, train_set = featuresets[:test_size], featuresets[test_size:]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(nltk.classify.accuracy(classifier, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   china = True           Republ : Democr =     27.1 : 1.0\n",
      "                   votes = True           Democr : Republ =     23.8 : 1.0\n",
      "             enforcement = True           Republ : Democr =     21.5 : 1.0\n",
      "                 destroy = True           Republ : Democr =     19.2 : 1.0\n",
      "                freedoms = True           Republ : Democr =     18.2 : 1.0\n",
      "                 climate = True           Democr : Republ =     17.8 : 1.0\n",
      "                supports = True           Republ : Democr =     17.1 : 1.0\n",
      "                   crime = True           Republ : Democr =     16.1 : 1.0\n",
      "                   media = True           Republ : Democr =     15.8 : 1.0\n",
      "                 beliefs = True           Republ : Democr =     13.0 : 1.0\n",
      "               countries = True           Republ : Democr =     13.0 : 1.0\n",
      "                 defense = True           Republ : Democr =     13.0 : 1.0\n",
      "                  defund = True           Republ : Democr =     13.0 : 1.0\n",
      "                    isis = True           Republ : Democr =     13.0 : 1.0\n",
      "                 liberal = True           Republ : Democr =     13.0 : 1.0\n",
      "                religion = True           Republ : Democr =     13.0 : 1.0\n",
      "                   trade = True           Republ : Democr =     12.7 : 1.0\n",
      "                    flag = True           Republ : Democr =     12.1 : 1.0\n",
      "               greatness = True           Republ : Democr =     12.1 : 1.0\n",
      "                 abraham = True           Republ : Democr =     11.9 : 1.0\n",
      "                    drug = True           Republ : Democr =     10.9 : 1.0\n",
      "              department = True           Republ : Democr =     10.9 : 1.0\n",
      "               destroyed = True           Republ : Democr =     10.9 : 1.0\n",
      "                   enemy = True           Republ : Democr =     10.9 : 1.0\n",
      "               amendment = True           Republ : Democr =     10.3 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a little prose here about what you see in the classifier. Anything odd or interesting?\n",
    "\n",
    "The first thing for me was that the naive bayes results were slightly worse than a random coin flip manking the accuracy unreliable in distinguishing between politiical party. The concept is still interesting for me as this could be used to promote desired candidate by best understanding what supporters prefer. \n",
    "\n",
    "### My Observations\n",
    "\n",
    "_Your observations to come._\n",
    "\n",
    "Displaying the 25 most important features present in a tweet alongside the ratio in my opionion is one of the strongest assests. To me personally, I could see these as key words to reference during a political debate knowing the value to a given party contrary to the opposing party. Developing marketing and promotional efforts to strategize capturing the most support from a given party. However, this could also be leveraged in the opposing sense to capture votes from opposing voters by stating buzz words that can appeal to them. The challenge is also to see the balance or lack thereof between the parties. There is a possibility that a given political party is more prominent on certain social platforms cauisng the poor classification model performance. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Classifying Congressional Tweets\n",
    "\n",
    "In this part we apply the classifer we just built to a set of tweets by people running for congress\n",
    "in 2018. These tweets are stored in the database `congressional_data.db`. That DB is funky, so I'll\n",
    "give you the query I used to pull out the tweets. Note that this DB has some big tables and \n",
    "is unindexed, so the query takes a minute or two to run on my machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cong_db = sqlite3.connect(\"congressional_data.db\")\n",
    "cong_cur = cong_db.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in the database:\n",
      "websites\n",
      "candidate_data\n",
      "tweets\n",
      "\n",
      "Columns in table websites:\n",
      "district (TEXT)\n",
      "candidate (TEXT)\n",
      "pull_time (DATETIME)\n",
      "url (TEXT)\n",
      "site_text (TEXT)\n",
      "\n",
      "Columns in table candidate_data:\n",
      "index (INTEGER)\n",
      "student (TEXT)\n",
      "state (TEXT)\n",
      "district_num (TEXT)\n",
      "formatted_dist_num (INTEGER)\n",
      "abbrev (TEXT)\n",
      "district (TEXT)\n",
      "candidate (TEXT)\n",
      "party (TEXT)\n",
      "website (TEXT)\n",
      "twitter_handle (TEXT)\n",
      "incumbent (TEXT)\n",
      "age (REAL)\n",
      "gender (TEXT)\n",
      "marital_status (TEXT)\n",
      "white_non_hispanic (TEXT)\n",
      "hispanic (TEXT)\n",
      "black (TEXT)\n",
      "partisian_lean_pvi (TEXT)\n",
      "opposed (TEXT)\n",
      "pct_urban (TEXT)\n",
      "income (REAL)\n",
      "region (TEXT)\n",
      "\n",
      "Columns in table tweets:\n",
      "district (TEXT)\n",
      "candidate (TEXT)\n",
      "pull_time (DATETIME)\n",
      "tweet_time (DATETIME)\n",
      "handle (TEXT)\n",
      "is_retweet (INTEGER)\n",
      "tweet_id (TEXT)\n",
      "tweet_text (TEXT)\n",
      "likes (INTEGER)\n",
      "replies (INTEGER)\n",
      "retweets (INTEGER)\n",
      "tweet_ratio (REAL)\n"
     ]
    }
   ],
   "source": [
    "# Connect to database displaying all tables & columns\n",
    "try:\n",
    "    # List all tables in the database\n",
    "    tables_query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "    cong_cur.execute(tables_query)\n",
    "    tables = cong_cur.fetchall()\n",
    "\n",
    "    # Check if tables are found\n",
    "    if not tables:\n",
    "        print(\"No tables found in the database.\")\n",
    "    else:\n",
    "        print(\"Tables in the database:\")\n",
    "        for table in tables:\n",
    "            print(table[0])\n",
    "\n",
    "        # Describe each table & column names\n",
    "        for table in tables:\n",
    "            print(f\"\\nColumns in table {table[0]}:\")\n",
    "            cong_cur.execute(f\"PRAGMA table_info({table[0]});\")\n",
    "            columns = cong_cur.fetchall()\n",
    "            for column in columns:\n",
    "                print(f\"{column[1]} ({column[2]})\")\n",
    "\n",
    "except sqlite3.Error as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "finally:\n",
    "    if cong_db:\n",
    "        cong_db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cong_db = sqlite3.connect(\"congressional_data.db\")\n",
    "cong_cur = cong_db.cursor()\n",
    "\n",
    "results = cong_cur.execute(\n",
    "        '''\n",
    "           SELECT DISTINCT \n",
    "                  cd.candidate, \n",
    "                  cd.party,\n",
    "                  tw.tweet_text\n",
    "           FROM candidate_data cd \n",
    "           INNER JOIN tweets tw ON cd.twitter_handle = tw.handle \n",
    "               AND cd.candidate == tw.candidate \n",
    "               AND cd.district == tw.district\n",
    "           WHERE cd.party in ('Republican','Democratic') \n",
    "               AND tw.tweet_text NOT LIKE '%RT%'\n",
    "        ''')\n",
    "\n",
    "results = list(results) # Just to store it, since the query is time consuming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean and tokenize text\n",
    "def clean_tokenize(text):\n",
    "    # Ensure text is decoded as UTF-8\n",
    "    text = text.decode('utf-8')\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    # Convert to lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    # Remove punctuation\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    # Return tokens as a single string\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data = []\n",
    "\n",
    "# Now fill up tweet_data with sublists like we did on the convention speeches.\n",
    "# Note that this may take a bit of time, since we have a lot of tweets.\n",
    "\n",
    "for row in results:\n",
    "    # Clean and tokenize the tweet text\n",
    "    cleaned_text = clean_tokenize(row[2])\n",
    "    # Extract the party affiliation\n",
    "    party = row[1]\n",
    "    # Append a list containing the cleaned text and party to tweet_data\n",
    "    tweet_data.append([cleaned_text, party])\n",
    "\n",
    "# Close the database connection\n",
    "cong_db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of tweets here. Let's take a random sample and see how our classifer does. I'm guessing it won't be too great given the performance on the convention speeches..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['earlier today spoke house floor abt protecting health care women praised ppmarmonte work central coast https',\n",
       "  'Democratic'],\n",
       " ['go tribe rallytogether https', 'Democratic'],\n",
       " ['apparently trump thinks easy students overwhelmed crushing burden debt pay student loans trumpbudget https',\n",
       "  'Democratic'],\n",
       " ['grateful first responders rescue personnel firefighters police volunteers working tirelessly keep people safe provide help putting lives line https',\n",
       "  'Republican'],\n",
       " ['let make even greater kag https', 'Republican'],\n",
       " ['cavs tie series repbarbaralee scared roadtovictory', 'Democratic'],\n",
       " ['congrats belliottsd new gig sd city hall glad continue https',\n",
       "  'Democratic'],\n",
       " ['really close raised toward match right whoot majors room help us get https https',\n",
       "  'Democratic'],\n",
       " ['today comment period potus plan expand offshore drilling opened public days march share oppose proposed program directly trump administration comments made email mail https',\n",
       "  'Democratic'],\n",
       " ['celebrated icseastla years eastside commitment amp saluted community leaders last night awards dinner https',\n",
       "  'Democratic']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(20201014) # Clever seed number. Bravo\n",
    "\n",
    "tweet_data_sample = random.choices(tweet_data,k=10)\n",
    "\n",
    "# Preview output\n",
    "tweet_data_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's our (cleaned) tweet: earlier today spoke house floor abt protecting health care women praised ppmarmonte work central coast https\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: go tribe rallytogether https\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: apparently trump thinks easy students overwhelmed crushing burden debt pay student loans trumpbudget https\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: grateful first responders rescue personnel firefighters police volunteers working tirelessly keep people safe provide help putting lives line https\n",
      "Actual party is Republican and our classifer says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: let make even greater kag https\n",
      "Actual party is Republican and our classifer says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: cavs tie series repbarbaralee scared roadtovictory\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "\n",
      "Here's our (cleaned) tweet: congrats belliottsd new gig sd city hall glad continue https\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: really close raised toward match right whoot majors room help us get https https\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: today comment period potus plan expand offshore drilling opened public days march share oppose proposed program directly trump administration comments made email mail https\n",
      "Actual party is Democratic and our classifer says Democratic.\n",
      "\n",
      "Here's our (cleaned) tweet: celebrated icseastla years eastside commitment amp saluted community leaders last night awards dinner https\n",
      "Actual party is Democratic and our classifer says Republican.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for tweet, party in tweet_data_sample :\n",
    "    estimated_party = random.choice(['Republican', 'Democratic'])\n",
    "    # Fill in the right-hand side above with code that estimates the actual party\n",
    "\n",
    "    print(f\"Here's our (cleaned) tweet: {tweet}\")\n",
    "    print(f\"Actual party is {party} and our classifer says {estimated_party}.\")\n",
    "    print(\"\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've looked at it some, let's score a bunch and see how we're doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of counts by actual party and estimated party.\n",
    "# The first key is the actual party, and the second is the estimated party.\n",
    "parties = ['Republican', 'Democratic']\n",
    "results = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "for p in parties:\n",
    "    for p1 in parties:\n",
    "        results[p][p1] = 0\n",
    "\n",
    "num_to_score = 10000\n",
    "random.shuffle(tweet_data)\n",
    "\n",
    "for idx, tp in enumerate(tweet_data):\n",
    "    tweet, party = tp    \n",
    "    # Now do the same thing as above, but we store the results rather\n",
    "    # than printing them. \n",
    "    \n",
    "    # Randomly estimate the party\n",
    "    estimated_party = random.choice(['Republican', 'Democratic'])\n",
    "    \n",
    "    results[party][estimated_party] += 1\n",
    "    \n",
    "    if idx > num_to_score:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>()>,\n",
       "            {'Republican': defaultdict(int,\n",
       "                         {'Republican': 2119, 'Democratic': 2180}),\n",
       "             'Democratic': defaultdict(int,\n",
       "                         {'Republican': 2811, 'Democratic': 2892})})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflections\n",
    "\n",
    "As indicated by the name, the results variable displaying just that, the results of the classification model in correctly labeling political parties. Starting with the Republican classifciation, there are 4,299 total posts while the model had an accuracy of approximately 49.3% which is slightly worse than a random coin flip. Similarly, there were 5,703 Democratic tweets with the model correctly predicting 50.7% of them correctly. While the concept and theoretical application has potential, there needs to be performance improvements prior to deploying this model.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
